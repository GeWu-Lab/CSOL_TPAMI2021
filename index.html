<!DOCTYPE html>
<html>
  <head>
    <title>Class-aware Sounding Objects Localization</title>
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css" integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
    <link rel="stylesheet" href="./css/main.css">
  </head>

  <body>
<div class="container">
  <table border="0" align="center">
    <tbody><tr>
      <td width="1000" align="center" valign="middle"><h1>Class-aware Sounding Objects Localization</h1>
        <h3>TPAMI 2021</h3></td>
    </tr>
    <tr>
      <td colspan="6" align="center"><h6> <a href="https://dtaoo.github.io">Di Hu</a>, <a href="https://echo0409.github.io/">Yake Wei</a>, <a href="https://shvdiwnkozbw.github.io/">Rui Qian</a>, <a href="https://weiyaolin.github.io/">Weiyao Lin</a>, <a href="https://scholar.google.com/citations?user=v5LctN8AAAAJ&hl">Ruihua Song</a>, <a href="https://scholar.google.com/citations?user=tbxCHJgAAAAJ">Ji-Rong Wen</a></h6></td>
      </tr>
      <tr>
        <td colspan="6" align="center"><h6><a href="https://arxiv.org/abs/2112.11749" target="_blank"> Download Paper</a>,  <a href="https://github.com/GeWu-Lab/CSOL_TPAMI2021" target="_blank"> Code & Dataset</a>, <a href="https://www.bilibili.com/video/BV1H3411L72Q?spm_id_from=333.999.0.0" target="_blank"> Video Tutorial[Chinese version] </a> </h6></td>
        </tr>

    </tbody></table>
</div>

<br>
    
<!-- <div class="container">
  <div class="imgDiv"><img class="cvprImg" src="https://convaistorageui.blob.core.windows.net/images/cvpr_tutorial/CVPR_image.png" /></div>
</div> -->

<br>

<div class="container">
  <h2>Abstract</h2>
  <p><img src='fig/teaser.jpg', align="right", width=35% > Audiovisual scenes are pervasive in our daily life. It is commonplace for humans to discriminatively localize different sounding objects but quite challenging for machines to achieve class-aware sounding objects localization without category annotations, i.e., localizing the sounding object and recognizing its category. To address this problem, we propose a two-stage step-by-step learning framework to localize and recognize sounding objects in complex audiovisual scenarios using only the correspondence between audio and vision. First, we propose to determine the sounding area via coarse-grained audiovisual correspondence in the single source cases. Then visual features in the sounding area are leveraged as candidate object representations to establish a category-representation object dictionary for expressive visual character extraction. We generate class-aware object localization maps in cocktail-party scenarios and use audiovisual correspondence to suppress silent areas by referring to this dictionary. Finally, we employ category-level audiovisual consistency as the supervision to achieve fine-grained audio and sounding object distribution alignment. Experiments on both realistic and synthesized videos show that our model is superior in localizing and recognizing objects as well as filtering out silent ones. We also transfer the learned audiovisual network into the unsupervised object detection task, obtaining reasonable performance.</p>
</div>
<br>
<br>



<div class="container">
  <h2>Video Demo</h2>
  <video width="60%" height="50%" controls>
    <source src="pami.mp4" type="video/mp4">
  </video>

</div>


<br>
<br>


<div class="container">
  <h2>Music Scenes</h2>
  <p>Music scenes with multiple-sources are common in our daily life. We first conduct expreiments on seceral realistic and synthetic dataset and visualize some localization oo cocktail-party videos. Our model can localize objects of different classes and filter out silent ones. The green box indicates target sounding object area, and the red box means this class of object is silent, and its activation value should be low.
  
    <figure>
      <img align="center" width="80%"
      src="fig/music.png"
      alt="Music Scenes.">
    </figure>
  </p>

</div>



<br>
<br>


<div class="container">
  <h2>General Scenes</h2>
  <p>Besides the music scenes, we expand our method to more general daily cases covering various sounding objects, e.g., humans, animals, vehicles. We also conduct expreiments on seceral real and synthetic dataset covering more general scenes and visualize some localization results.
    <figure>
      <img align="center" width="80%"
      src="fig/gene.png"
      alt="Genral Scenes.">
    </figure>
  </p>

</div>

<br>
<br>
<div class="container">
  <h2>Application: Unsupervised Object Detection</h2>
  <p>In this work, we utilize the correspondence between audio and vision to perform class-aware sounding objects localization. And the model can learn vision knowledge of different objects during the training of sounding object localization. Then, we propose to transfer such knowledge into typical vision tasks, like object detection. Concretely, the obtained localization heatmap is then used to generate bounding box. The generated bounding box and the cluster label are utilized to train a Faster-RCNN model. Based on the experiment results, our method is able to distinguish different objects and generate bounding boxes effectively.
    <figure>
      <img align="center" width="80%"
      src="fig/detection.png"
      alt="Detection.">
    </figure>
  </p>

</div>

<br>
<br>

<div class="container">
  <h2>Dataset</h2>
  <p>
    <strong><a href="https://zenodo.org/record/4079386#.X4PFodozbb2" target="_blank"> MUSIC-Synthetic </a>& <a href="https://zenodo.org/record/6237158#.YhXUFZNBzao" target="_blank"> VGGSound-Synthetic</a>: </strong>To help the learning and evaluation of multiple-sources sound localization in the cocktail-party scenario, we build multi-source videos by artificially synthesizing solo videos from the MUSIC and VGGSound dataset, repectively. Go in detail, we first randomly select four 1-second solo audiovisual pairs of different categories, then get the multi-source video frames by means of mixing random two audio clips with jittering, and multi-source video frame is synthesized through concatenating four frames of these clips. That means there are two sounding instruments and two silent instruments in the synthesized audiovisual pair. Thus, this synthesized dataset is quite appropriate for the evaluation of class-aware sounding object localization. 
  </p>
  <p>
    <strong><a href="https://zenodo.org/record/6235313#.YhWmRZNBw-Q" target="_blank"> Realistic MUSIC </a>& <a href="https://zenodo.org/record/6235340#.YhWmtpNBw-Q" target="_blank"> DailyLife</a>: </strong>To further evaluate our model in real-world scenes, we collect real multi-source instrument videos and real multi-source daily life scenes on YouTube. Each instrument video includes 3 to 4 instruments and each daily life scene includes 2 events Because of the high noise of the real data, we perform manual labeling for bounding box annotation. We select the first second from each video and manually label the corresponding frame, marking out the bounding boxes and whether each object is sounding or not.
  </p>
  <p>For more details about the dataset, please refer to our paper.</p>

</div>


<br>
<br>

<div class="container">
  <h2>Bibtex</h2>
  <p>If you find our work useful in your research, please cite:
  </p>
  <pre>
    @article{hu2021class,
      title={Class-aware Sounding Objects Localization via Audiovisual Correspondence},
      author={Hu, Di and Wei, Yake and Qian, Rui and Lin, Weiyao and Song, Ruihua and Wen, Ji-Rong},
      journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
      year={2021}
    }
  </pre>

</div>




<!--<p align="center" class="acknowledgement">Last updated: 30 July 2012</p>-->

<script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js" integrity="sha384-JjSmVgyd0p3pXB1rRibZUAYoIIy6OrQ6VrjIEaFf/nJGzIxFDsf4x0xIM+B07jRM" crossorigin="anonymous"></script>

<script>
//openCity(event, 'PST');
document.getElementById("default_button").click(); // Click on the checkbox
function openCity(evt, cityName) {
  // Declare all variables
  var i, tabcontent, tablinks;

  // Get all elements with class="tabcontent" and hide them
  tabcontent = document.getElementsByClassName("tabcontent");
  for (i = 0; i < tabcontent.length; i++) {
    tabcontent[i].style.display = "none";
  }

  // Get all elements with class="tablinks" and remove the class "active"
  tablinks = document.getElementsByClassName("tablinks");
  for (i = 0; i < tablinks.length; i++) {
    tablinks[i].className = tablinks[i].className.replace(" active", "");
  }

  // Show the current tab, and add an "active" class to the button that opened the tab
  document.getElementById(cityName).style.display = "block";
  evt.currentTarget.className += " active";
}

</script>

</body></html>
